{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7] Amazon Fine Food Reviews Analysis\n",
    "\n",
    "\n",
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n",
    "\n",
    "Number of reviews: 568,454<br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10 \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "\n",
    "#### Objective:\n",
    "Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "<br>\n",
    "[Q] How to determine if a review is positive or negative?<br>\n",
    "<br> \n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7.1] Loading the data\n",
    "\n",
    "The dataset is available in two forms\n",
    "1. .csv file\n",
    "2. SQLite Database\n",
    "\n",
    "In order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n",
    "<br> \n",
    "\n",
    "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import database related packages\n",
    "import sqlite3\n",
    "\n",
    "# import text processing related packages\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# W2V Related Packages\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# for saving model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# regular expression for handling strings\n",
    "import scipy.sparse\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Visualization related packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Label</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Label        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('/media/amd_3/20DAD539DAD50BC2/DSET_REPO/DataSets/CS01-AMZN_FOOD_REVIEW/database.sqlite') \n",
    "\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con) \n",
    "\n",
    "# Give reviews with Score>3 a positive rating (1), and reviews with a score<3 a negative rating (0).\n",
    "filtered_data['Score'] = filtered_data['Score'].apply(lambda x : 1 if x > 3  else 0)\n",
    "filtered_data.rename(columns={'Score' : 'Label'}, inplace=True)\n",
    "\n",
    "print(filtered_data.shape) #looking at the number of attributes and size of the data\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exploratory Data Analysis\n",
    "\n",
    "## [7.1.2] Data Cleaning: Deduplication\n",
    "\n",
    "It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.  Following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above the same user has multiple reviews of the with the same values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text  and on doing analysis it was found that <br>\n",
    "<br> \n",
    "ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)<br>\n",
    "<br> \n",
    "ProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on<br>\n",
    "\n",
    "It was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.<br>\n",
    "\n",
    "The method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others. for eg. in the above just the review for ProductId=B000HDL1RQ remains. This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:-</b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\", con)\n",
    "\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.3  Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare a stemmer & set of stopwords to remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words with more than two letters 142\n"
     ]
    }
   ],
   "source": [
    "# Global declaration of two variables\n",
    "\n",
    "# set a stemmer for finding root words\n",
    "snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# get a set of stop words in english, 3 common stop words are excluded from this set because\n",
    "# it helps to identify the customer liked the product or not, less than length 3 stop words \n",
    "# are eliminated from this list because all words with length less than 3 are removed just \n",
    "# before applying stop word removal\n",
    "stop_words_set = set(stopwords.words('english')) - {'not', 'too', 'very'}\n",
    "stop_words_set = set(filter(lambda x : len(x) > 2, stop_words_set))\n",
    "print('Total stop words with more than two letters', len(stop_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract_phrase(phrase):\n",
    "    # https://stackoverflow.com/a/47091490/4084039\n",
    "    \"\"\"\n",
    "    This function decontracts the common phrases into its full form.\n",
    "    \"\"\"\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_letter(word):\n",
    "    \"\"\"\n",
    "    This function truncates repeated letters (letters that occur in consequtive locations more than 3\n",
    "    three times into 3 letters.\n",
    "    \n",
    "    eg: 'The price is tooooooooooooooooooooooooooooooooooooooooooooooooooo high !!!!' will be converted\n",
    "    to 'The price is too high !!!'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    word_size = len(word)\n",
    "    \n",
    "    # if the word size is less than three return as it is\n",
    "    if word_size < 3:\n",
    "        return word\n",
    "    \n",
    "    # consider first two letter of the word as it is\n",
    "    new_word_letters_list = list(word[0:2])\n",
    "\n",
    "    # for every letter starting from the third position\n",
    "    for letter in word[2:]:\n",
    "        \n",
    "        # case 1: the current letter we are adding is a duplicate \n",
    "        if (letter == new_word_letters_list[-1]) and (letter == new_word_letters_list[-2]):\n",
    "            continue\n",
    "            \n",
    "        # case 2: the current letter we are adding is not a duplicate\n",
    "        new_word_letters_list.append(letter)\n",
    "        \n",
    "\n",
    "    return ''.join(new_word_letters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_str):\n",
    "    \"\"\"\n",
    "    This function takes a raw text and clean it using various operations like removing html tags,\n",
    "    removing special characters, removing stop words etc and returns the cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### CLEAN the test ################################\n",
    "    \n",
    "    # step 1: Remove HTML tags\n",
    "    html_cleanr = re.compile('<.*?>')\n",
    "    text_str = re.sub(html_cleanr, ' ', text_str)\n",
    "    \n",
    "    # step 2: Convert to lower case letters\n",
    "    text_str = text_str.lower()\n",
    "    \n",
    "    # step 3 : Decontract the word\n",
    "    text_str = decontract_phrase(text_str)\n",
    "    \n",
    "    # step 4: Remove any consequitve occurence of letter in a word & also remove multiple \n",
    "    # spaces with a single space\n",
    "    text_str = ' '.join(list(map(remove_repeated_letter, text_str.split())))\n",
    "    \n",
    "    # step 5: Remove special characters\n",
    "    text_str = re.sub(r'[^a-z ]', r' ', text_str)\n",
    "    \n",
    "    # convert text into list of words\n",
    "    text_words_list = text_str.split()\n",
    "    \n",
    "    # step 6: Remove all words with size less than three\n",
    "    text_words_list = list(filter(lambda x : len(x) > 2, text_words_list))\n",
    "    \n",
    "    # step 7: remove unwanted stop words decalred in the global set of stop words\n",
    "    text_words_list = list(filter(lambda x : x not in stop_words_set , text_words_list))\n",
    "    \n",
    "    # step 8: Stemming of words to find the root word\n",
    "    text_words_list = list(map(snowball_stemmer.stem, text_words_list))\n",
    "         \n",
    "    # step 9 : join all words with space\n",
    "    text_str = ' '.join(text_words_list)\n",
    "    \n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adsffdfad price too dee not much adfdsf good love like'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"\"\"adsFFFdfad <body> price is              tooooooooooo deeeeeee\\'t Much!!! </body>adfdsf t34 34 Good! \n",
    "            loved ### $$$ it liking !!!!!!1\"\"\"\n",
    "clean_text(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning size: (364171, 11)\n",
      "After empty review removal size: (364169, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>939340800</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>witti littl book make son laugh loud recit car...</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>940809600</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>rememb see show air televis year ago child sis...</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346055</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>944438400</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417838</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>946857600</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId       Time  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL  939340800   \n",
       "138683  150501  0006641040   AJ46FKXOVC7NR  940809600   \n",
       "417839  451856  B00004CXX9   AIUWLEQ1ADEG5  944092800   \n",
       "346055  374359  B00004CI84  A344SMIA5JECGM  944438400   \n",
       "417838  451855  B00004CXX9   AJH6LUC1UT1ON  946857600   \n",
       "\n",
       "                     ProfileName  HelpfulnessNumerator  \\\n",
       "138706           shari zychinski                     0   \n",
       "138683        Nicholas A Mesiano                     2   \n",
       "417839          Elizabeth Medina                     0   \n",
       "346055           Vincent P. Ross                     1   \n",
       "417838  The Phantom of the Opera                     0   \n",
       "\n",
       "        HelpfulnessDenominator  \\\n",
       "138706                       0   \n",
       "138683                       2   \n",
       "417839                       0   \n",
       "346055                       2   \n",
       "417838                       0   \n",
       "\n",
       "                                              CleanedText  \\\n",
       "138706  witti littl book make son laugh loud recit car...   \n",
       "138683  rememb see show air televis year ago child sis...   \n",
       "417839  beetlejuic well written movi everyth excel act...   \n",
       "346055  twist rumplestiskin captur film star michael k...   \n",
       "417838  beetlejuic excel funni movi keaton hilari wack...   \n",
       "\n",
       "                                                  Summary  Label  \n",
       "138706                          EVERY book is educational      1  \n",
       "138683  This whole series is great way to spend time w...      1  \n",
       "417839                               Entertainingl Funny!      1  \n",
       "346055                            A modern day fairy tale      1  \n",
       "417838                                         FANTASTIC!      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['CleanedText'] = final['Text'].apply(clean_text)\n",
    "print('After cleaning size:', final.shape)\n",
    "\n",
    "# remove any empty review rows presnet after cleaning operation. The size of review should be atleast 3 letter.\n",
    "final = final[final['CleanedText'].apply(len) > 2]\n",
    "print('After empty review removal size:', final.shape)\n",
    "\n",
    "# keep only the required columns\n",
    "final = final[['Id', 'ProductId', 'UserId', 'Time', 'ProfileName', 'HelpfulnessNumerator',\n",
    "               'HelpfulnessDenominator', 'CleanedText', 'Summary', 'Label']]\n",
    "# sort the data frame in ascending order of time stamp, this is for time based partitioning\n",
    "final = final.sort_values(['Time'])\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store final table into an SQlLite table for future.\n",
    "final_db_path = '/home/amd_3/AAIC/ASM_REPO/Processed_data/AMZN_FOOD_REVIW/final.sqlite'\n",
    "\n",
    "conn = sqlite3.connect(final_db_path)\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews', conn,  schema=None, if_exists='replace', index=False, \n",
    "             index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the final Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "      <th>Review_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>939340800</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>witti littl book make son laugh loud recit car...</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>940809600</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>rememb see show air televis year ago child sis...</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>944438400</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>946857600</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId       Time               ProfileName  \\\n",
       "0  150524  0006641040   ACITT7DI6IDDL  939340800           shari zychinski   \n",
       "1  150501  0006641040   AJ46FKXOVC7NR  940809600        Nicholas A Mesiano   \n",
       "2  451856  B00004CXX9   AIUWLEQ1ADEG5  944092800          Elizabeth Medina   \n",
       "3  374359  B00004CI84  A344SMIA5JECGM  944438400           Vincent P. Ross   \n",
       "4  451855  B00004CXX9   AJH6LUC1UT1ON  946857600  The Phantom of the Opera   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "0                     0                       0   \n",
       "1                     2                       2   \n",
       "2                     0                       0   \n",
       "3                     1                       2   \n",
       "4                     0                       0   \n",
       "\n",
       "                                         CleanedText  \\\n",
       "0  witti littl book make son laugh loud recit car...   \n",
       "1  rememb see show air televis year ago child sis...   \n",
       "2  beetlejuic well written movi everyth excel act...   \n",
       "3  twist rumplestiskin captur film star michael k...   \n",
       "4  beetlejuic excel funni movi keaton hilari wack...   \n",
       "\n",
       "                                             Summary  Label  Review_Length  \n",
       "0                          EVERY book is educational      1             35  \n",
       "1  This whole series is great way to spend time w...      1             33  \n",
       "2                               Entertainingl Funny!      1             13  \n",
       "3                            A modern day fairy tale      1             21  \n",
       "4                                         FANTASTIC!      1             25  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read back the data\n",
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect(final_db_path) \n",
    "\n",
    "final_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews\"\"\", con) \n",
    "# add review length\n",
    "final_data['Review_Length'] = final_data['CleanedText'].apply(lambda x: len(x.split()))\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364169, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    202803\n",
       "0     34997\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[0:237800]['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(final_df):\n",
    "\n",
    "    # consider first 237800 points for generating train sample and remaining for  test samples.\n",
    "    # within 237800 points we have 35000 - ve samples and others are +ve, from this set we\n",
    "    # can take a sample of 35000 +ve, so we will have a balanced data set having 35K +ve, 35K -ve\n",
    "    # points which is apt for training the model\n",
    "\n",
    "    # partiton the data for train, test data set generation\n",
    "    final_df_train = final_df[0:237800]\n",
    "    final_df_test = final_df[237800:]\n",
    "\n",
    "    # partition the data frame to positive and negative\n",
    "    final_df_positive = final_df_train[final_df_train['Label'] == 1]\n",
    "    final_df_negative = final_df_train[final_df_train['Label'] == 0]\n",
    "\n",
    "    # since positive sample is dominating we select 30K samples randomly from the positive partiion &\n",
    "    #  take whole negative samples\n",
    "    final_df_positive = final_df_positive.sample(n=35000)\n",
    "\n",
    "    # form train sample set\n",
    "    final_train_df = final_df_positive.append(final_df_negative)\n",
    "    final_train_df = final_train_df.sample(frac=1.0)\n",
    "    final_train_df = final_train_df.reset_index(drop=True)\n",
    "\n",
    "    # sample 30K points for testing\n",
    "    final_test_df = final_df_test.sample(n=30000)\n",
    "    final_test_df = final_test_df.reset_index(drop=True)\n",
    "\n",
    "    print('Final train df statistics:\\n', final_train_df['Label'].value_counts())\n",
    "    print('\\n\\nFinal test df statistics:\\n', final_test_df['Label'].value_counts())\n",
    "    \n",
    "    return (final_train_df, final_test_df,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train df statistics:\n",
      " 1    35000\n",
      "0    34997\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "Final test df statistics:\n",
      " 1    24754\n",
      "0     5246\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "final_train_df, final_test_df = get_train_test_split(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "      <th>Review_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456873</td>\n",
       "      <td>B000LKTZSM</td>\n",
       "      <td>AY6TK80W3N0KF</td>\n",
       "      <td>1266451200</td>\n",
       "      <td>Rutherford</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>chose tomato label organ sinc discov muir glen...</td>\n",
       "      <td>Dishonest labeling of \"organic\" when cans cont...</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81416</td>\n",
       "      <td>B001JTIFUI</td>\n",
       "      <td>A215TX3PTHM32D</td>\n",
       "      <td>1295395200</td>\n",
       "      <td>Beth \"Beth\"</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>nasti chemic aftertast smell artifici disappoi...</td>\n",
       "      <td>Undrinkable</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>519340</td>\n",
       "      <td>B003VD9MPW</td>\n",
       "      <td>A2TLXWT5XDS9PX</td>\n",
       "      <td>1299542400</td>\n",
       "      <td>Karla Robinett</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>not think would like decaf starbuck decid give...</td>\n",
       "      <td>Best Decaf Ever</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340949</td>\n",
       "      <td>B000UJTZ8O</td>\n",
       "      <td>A2BPUJL5ZO011X</td>\n",
       "      <td>1313712000</td>\n",
       "      <td>Mr. Johnson</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trap hard set trip easili push ground veri fru...</td>\n",
       "      <td>Junk Do not buy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>453782</td>\n",
       "      <td>B00684ILVW</td>\n",
       "      <td>AV4MG4PDBLD0H</td>\n",
       "      <td>1325635200</td>\n",
       "      <td>Barbaramom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>order item famili receiv contain one pig error</td>\n",
       "      <td>Item stated 2 only received 1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId        Time     ProfileName  \\\n",
       "0  456873  B000LKTZSM   AY6TK80W3N0KF  1266451200      Rutherford   \n",
       "1   81416  B001JTIFUI  A215TX3PTHM32D  1295395200     Beth \"Beth\"   \n",
       "2  519340  B003VD9MPW  A2TLXWT5XDS9PX  1299542400  Karla Robinett   \n",
       "3  340949  B000UJTZ8O  A2BPUJL5ZO011X  1313712000     Mr. Johnson   \n",
       "4  453782  B00684ILVW   AV4MG4PDBLD0H  1325635200      Barbaramom   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "0                     3                       7   \n",
       "1                     0                       1   \n",
       "2                     1                       1   \n",
       "3                     0                       0   \n",
       "4                     0                       0   \n",
       "\n",
       "                                         CleanedText  \\\n",
       "0  chose tomato label organ sinc discov muir glen...   \n",
       "1  nasti chemic aftertast smell artifici disappoi...   \n",
       "2  not think would like decaf starbuck decid give...   \n",
       "3  trap hard set trip easili push ground veri fru...   \n",
       "4     order item famili receiv contain one pig error   \n",
       "\n",
       "                                             Summary  Label  Review_Length  \n",
       "0  Dishonest labeling of \"organic\" when cans cont...      0            136  \n",
       "1                                        Undrinkable      0             11  \n",
       "2                                    Best Decaf Ever      1             35  \n",
       "3                                    Junk Do not buy      0             10  \n",
       "4                      Item stated 2 only received 1      0              8  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Time</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "      <th>Review_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>401792</td>\n",
       "      <td>B0035QJARK</td>\n",
       "      <td>A34TQDJ94475AO</td>\n",
       "      <td>1330819200</td>\n",
       "      <td>Jay Endo \"Jay Endo\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>use fri fish catfish flounder even sea bass al...</td>\n",
       "      <td>Great Stuff</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210928</td>\n",
       "      <td>B0025VF8TK</td>\n",
       "      <td>A1N7ROYP7TGWI</td>\n",
       "      <td>1328054400</td>\n",
       "      <td>JoeSchmoe155</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>far open one let put way not know salmon would...</td>\n",
       "      <td>Doesn't look like a salmon, doesn't taste like...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>565701</td>\n",
       "      <td>B002GKEK7G</td>\n",
       "      <td>A18LM9AWCHBL8U</td>\n",
       "      <td>1341878400</td>\n",
       "      <td>Christy M.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weight loss fit blogger runner tri mani protei...</td>\n",
       "      <td>Best Protein Drink Out There!</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315160</td>\n",
       "      <td>B002BEKJUY</td>\n",
       "      <td>A22OUEFLM49JH6</td>\n",
       "      <td>1346198400</td>\n",
       "      <td>Steve and Dana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good eat straight bag believ chocol compani se...</td>\n",
       "      <td>Yummy</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277135</td>\n",
       "      <td>B004CJUE8I</td>\n",
       "      <td>A36XETPK7F42BY</td>\n",
       "      <td>1348704000</td>\n",
       "      <td>Just Falcon \"sf\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>love take packet travel great slushi twist lim...</td>\n",
       "      <td>Great in bottled water, you may not need the w...</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId        Time          ProfileName  \\\n",
       "0  401792  B0035QJARK  A34TQDJ94475AO  1330819200  Jay Endo \"Jay Endo\"   \n",
       "1  210928  B0025VF8TK   A1N7ROYP7TGWI  1328054400         JoeSchmoe155   \n",
       "2  565701  B002GKEK7G  A18LM9AWCHBL8U  1341878400           Christy M.   \n",
       "3  315160  B002BEKJUY  A22OUEFLM49JH6  1346198400       Steve and Dana   \n",
       "4  277135  B004CJUE8I  A36XETPK7F42BY  1348704000     Just Falcon \"sf\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "0                     0                       0   \n",
       "1                     2                       4   \n",
       "2                     0                       0   \n",
       "3                     0                       0   \n",
       "4                     0                       0   \n",
       "\n",
       "                                         CleanedText  \\\n",
       "0  use fri fish catfish flounder even sea bass al...   \n",
       "1  far open one let put way not know salmon would...   \n",
       "2  weight loss fit blogger runner tri mani protei...   \n",
       "3  good eat straight bag believ chocol compani se...   \n",
       "4  love take packet travel great slushi twist lim...   \n",
       "\n",
       "                                             Summary  Label  Review_Length  \n",
       "0                                        Great Stuff      1             20  \n",
       "1  Doesn't look like a salmon, doesn't taste like...      0             73  \n",
       "2                      Best Protein Drink Out There!      1             46  \n",
       "3                                              Yummy      1             10  \n",
       "4  Great in bottled water, you may not need the w...      1             43  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%HTML\n",
    "<ul>\n",
    "<li> The train sample is almost balanced </li>\n",
    "<li> The test sample is unbalanced like in the real case scenario (-ve : +ve = : 1: 5)</li>\n",
    "<li> Train Test split is done by the time based splitting method</li>\n",
    "<li> The cleaned text doesnt contain any unwanted letters</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7.2.4] Bi-Grams and n-Grams.\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "Now that we have our list of words describing positive and negative reviews lets analyse them.<br>\n",
    "\n",
    "We begin analysis by getting the frequency distribution of the words as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = final_data[final_data['Label'] == 1]\n",
    "negative_df = final_data[final_data['Label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched positvie words\n",
      "Fetched negative words\n"
     ]
    }
   ],
   "source": [
    "all_positive_words = list()\n",
    "for item in positive_df['CleanedText'].tolist():\n",
    "    all_positive_words += item.split()\n",
    "print('Fetched positvie words')\n",
    "\n",
    "all_negative_words = list()\n",
    "for item in negative_df['CleanedText'].tolist():\n",
    "    all_negative_words += item.split()\n",
    "print('Fetched negative words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words :  [('not', 289276), ('like', 141054), ('tast', 131285), ('good', 113985), ('flavor', 111640), ('love', 107744), ('great', 104580), ('use', 104344), ('one', 97741), ('product', 92283), ('veri', 91063), ('tri', 87145), ('tea', 84831), ('coffe', 79810), ('make', 75413), ('would', 72874), ('get', 72275), ('food', 65619), ('time', 56218), ('buy', 54320), ('realli', 52830), ('eat', 52304), ('amazon', 50036), ('price', 49479), ('too', 49288)]\n",
      "Most Common Negative Words :  [('not', 94104), ('tast', 35188), ('like', 32791), ('product', 28697), ('would', 23361), ('one', 20791), ('flavor', 20025), ('tri', 17796), ('veri', 17061), ('use', 15367), ('good', 15192), ('coffe', 14902), ('get', 13802), ('buy', 13771), ('order', 13022), ('food', 12914), ('tea', 11786), ('even', 11112), ('box', 10950), ('amazon', 10159), ('time', 9931), ('make', 9866), ('bag', 9838), ('eat', 9550), ('much', 9465)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(25))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:-</b> From the above it can be seen that the most common positive and the negative words overlap for eg. 'like' could be used as 'not like' etc. <br>\n",
    "So, it is a good idea to consider pairs of consequent words (bi-grams) or q sequnce of n consecutive words (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=500, min_df=0.001,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=0.001, max_df=0.95, max_features=500)\n",
    "\n",
    "# Fit on train data\n",
    "count_vect.fit(final_train_df['CleanedText'].values)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 500\n"
     ]
    }
   ],
   "source": [
    "print('Final vocabulary size:',len(count_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words which are ignored due to appearing in almost all documents or it is very rare 1004725\n"
     ]
    }
   ],
   "source": [
    "print('Number of Words which are ignored due to appearing in almost all documents or it is very rare', \n",
    "      len(count_vect.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all bigram features ['could not', 'flavor not', 'gluten free', 'green tea', 'groceri store', 'high recommend', 'much better', 'not buy', 'not eat', 'not even', 'not get', 'not good', 'not know', 'not like', 'not recommend', 'not sure', 'not tast', 'not too', 'peanut butter', 'product not', 'tast good', 'tast great', 'tast like', 'tast not', 'thought would', 'too much', 'veri disappoint', 'veri good', 'wast money', 'would not']\n"
     ]
    }
   ],
   "source": [
    "print('all bigram features', list(filter(lambda x: len(x.split()) > 1, count_vect.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%HTML\n",
    "<ul>\n",
    "<li> We got some interesting bigrams like not buy, high recommend, wast money, veri disappoint etc. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for saving the sparse matrix (BoW features n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bow_features(count_vect, df, partition_name):\n",
    "    \"\"\"\n",
    "    This function takes a data frame and featurize the text data using\n",
    "    bag of words method. This also saves the featurized data frame and the\n",
    "    bag of words model to specific location.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set output base directory\n",
    "    out_base_dir = '/home/amd_3/AAIC/ASM_REPO/Processed_data/AMZN_FOOD_REVIW/BOW/'\n",
    "    \n",
    "    # vectorize the data using BOW and save the sparse matrix\n",
    "    final_bigram_counts = count_vect.transform(df['CleanedText'].values)\n",
    "    \n",
    "    print(\"the type of count vectorizer \", type(final_bigram_counts))\n",
    "    print(\"the shape of out text BOW vectorizer \", final_bigram_counts.get_shape())\n",
    "    print(\"the number of unique words including both unigrams and bigrams \", \n",
    "          final_bigram_counts.get_shape()[1])\n",
    "    \n",
    "    # create a data frame for bow features\n",
    "    bow_bigram_bow = pd.DataFrame(final_bigram_counts.todense(), columns=count_vect.get_feature_names())\n",
    "    # add the review length as additional column\n",
    "    bow_bigram_bow['Review_Length'] = df['Review_Length']\n",
    "    bow_bigram_bow['Label'] = df['Label']\n",
    "    bow_bigram_bow['Id'] = df['Id']\n",
    "    \n",
    "    # write to disk\n",
    "    bow_bigram_bow.to_csv(os.path.join(out_base_dir, partition_name + '_bow_bigram.csv'),\n",
    "                          index=False)\n",
    "    \n",
    "    \n",
    "    # dump the bow model as pickle file\n",
    "    bow_model_file = open(os.path.join(out_base_dir, 'bow_model.pickle'), 'wb')\n",
    "    pickle.dump(count_vect, bow_model_file)\n",
    "    bow_model_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (69997, 500)\n",
      "the number of unique words including both unigrams and bigrams  500\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (30000, 500)\n",
      "the number of unique words including both unigrams and bigrams  500\n"
     ]
    }
   ],
   "source": [
    "# vectorize train, test data and save it\n",
    "save_bow_features(count_vect, final_train_df, 'train')\n",
    "save_bow_features(count_vect, final_test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.5] TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=0.001, max_df=0.95, max_features=500)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final_train_df['CleanedText'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_features(tfidf_vect, df, partition_name):\n",
    "    \"\"\"\n",
    "    This function takes a data frame and featurize the text data using\n",
    "    TF-IDF method. This also saves the featurized data frame and the\n",
    "    TF-IDF model to specific location.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set output base directory\n",
    "    out_base_dir = '/home/amd_3/AAIC/ASM_REPO/Processed_data/AMZN_FOOD_REVIW/TFIDF/'\n",
    "    \n",
    "    # vectorize the data using BOW and save the sparse matrix\n",
    "    final_bigram_tfidf = tfidf_vect.transform(df['CleanedText'].values)\n",
    "    \n",
    "    print(\"Type of count vectorizer \", type(final_bigram_tfidf))\n",
    "    print(\"Shape of out text BOW vectorizer \", final_bigram_tfidf.get_shape())\n",
    "    print(\"Number of unique words including both unigrams and bigrams \", \n",
    "          final_bigram_tfidf.get_shape()[1])\n",
    "    \n",
    "    # create a data frame for bow features\n",
    "    tfidf_df = pd.DataFrame(final_bigram_tfidf.todense(), columns=tfidf_vect.get_feature_names())\n",
    "    # add review length as additional column\n",
    "    tfidf_df['Review_Length'] = df['Review_Length']\n",
    "    tfidf_df['Label'] = df['Label']\n",
    "    tfidf_df['Id'] = df['Id']\n",
    "    \n",
    "    # write to disk\n",
    "    tfidf_df.to_csv(os.path.join(out_base_dir, partition_name + '_bigram_tfidf.csv'), \n",
    "                    index=False)\n",
    "                    \n",
    "    # dump the bow model as pickle file\n",
    "    tfidf_model_file = open(os.path.join(out_base_dir, 'tfidf_model.pickle'), 'wb')\n",
    "    pickle.dump(tfidf_vect, tfidf_model_file)\n",
    "    tfidf_model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of out text BOW vectorizer  (69997, 500)\n",
      "Number of unique words including both unigrams and bigrams  500\n",
      "Type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of out text BOW vectorizer  (30000, 500)\n",
      "Number of unique words including both unigrams and bigrams  500\n"
     ]
    }
   ],
   "source": [
    "# vectorize train, validation, test data and save it\n",
    "save_tfidf_features(tf_idf_vect, final_train_df, 'train')\n",
    "save_tfidf_features(tf_idf_vect, final_test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.6] Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [chose, tomato, label, organ, sinc, discov, mu...\n",
       "1    [nasti, chemic, aftertast, smell, artifici, di...\n",
       "2    [not, think, would, like, decaf, starbuck, dec...\n",
       "Name: CleanedText, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of list for training the word to vec model\n",
    "w2vec_train_data = final_train_df['CleanedText'].apply(lambda x : x.split())\n",
    "# print sample training data for w2v\n",
    "w2vec_train_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our own Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# consider only those words which appeared 5 times\n",
    "w2v_model = Word2Vec(w2vec_train_data, min_count=5, size=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  11211\n",
      "sample words  ['chose', 'tomato', 'label', 'organ', 'sinc', 'discov', 'muir', 'glen', 'can', 'contain', 'bpa', 'line', 'research', 'show', 'solid', 'level', 'chemic', 'leech', 'plastic', 'content', 'american', 'blood', 'stream', 'infant', 'elder', 'not', 'think', 'product', 'ethic', 'call', 'peopl', 'empti', 'food', 'direct', 'recip', 'without', 'wash', 'first', 'one', 'rins', 'broth', 'soup', 'way', 'fact', 'mani', 'like', 'salt', 'season', 'therefor', 'never']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yummi', 0.7804615497589111),\n",
       " ('delici', 0.7529192566871643),\n",
       " ('satisfi', 0.7077146768569946),\n",
       " ('nutriti', 0.6981334686279297),\n",
       " ('nice', 0.6340436935424805),\n",
       " ('versatil', 0.6295880079269409),\n",
       " ('tastey', 0.6231692433357239),\n",
       " ('crunchi', 0.6142654418945312),\n",
       " ('good', 0.5960522890090942),\n",
       " ('dens', 0.5911023616790771)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okay', 0.7673594951629639),\n",
       " ('weird', 0.7271952629089355),\n",
       " ('remind', 0.6655181646347046),\n",
       " ('gross', 0.6525304317474365),\n",
       " ('funki', 0.6406497955322266),\n",
       " ('alright', 0.640399158000946),\n",
       " ('appeal', 0.6389076709747314),\n",
       " ('resembl', 0.6381198167800903),\n",
       " ('funni', 0.6374648809432983),\n",
       " ('aw', 0.6307752132415771)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.7] Avg W2V Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_w2v_features(w2v_model, df, partition_type):\n",
    "    \n",
    "    # set output base directory\n",
    "    out_base_dir = '/home/amd_3/AAIC/ASM_REPO/Processed_data/AMZN_FOOD_REVIW/AVG_W2V/'\n",
    "    \n",
    "    # average Word2Vec\n",
    "    list_of_sent = (df['CleanedText'].apply(lambda x : x.split())).tolist()\n",
    "    \n",
    "    # compute average word2vec for each review.\n",
    "    sent_vectors = list() # the avg-w2v for each sentence/review is stored in this list\n",
    "    \n",
    "    # convert each review into vector format\n",
    "    for sent in list_of_sent: # for each review/sentence\n",
    "        \n",
    "        sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "        cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "        \n",
    "        for word in sent: # for each word in a review/sentence\n",
    "                \n",
    "            try:\n",
    "                vec = w2v_model.wv[word]\n",
    "                sent_vec += vec\n",
    "                cnt_words += 1\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if cnt_words != 0:\n",
    "            sent_vec /= cnt_words\n",
    "            \n",
    "        sent_vectors.append(sent_vec)\n",
    "        \n",
    "    # create a data frame\n",
    "    col_names = ['dim_' + str(item) for item in range(1, 51)]\n",
    "    feature_df = pd.DataFrame(sent_vectors, columns=col_names)\n",
    "    feature_df['Label'] = df['Label'].tolist()\n",
    "    feature_df['Id'] = df['Id']\n",
    "    \n",
    "    # write to disk\n",
    "    feature_df.to_csv(os.path.join(out_base_dir, partition_type +'_avg_w2v.csv'),\n",
    "                      index=False)\n",
    "    # dump the bow model as pickle file\n",
    "    w2v_model_file = open(os.path.join(out_base_dir, 'avg_w2v_model.pickle'), 'wb')\n",
    "    pickle.dump(w2v_model, w2v_model_file)\n",
    "    w2v_model_file.close()\n",
    "    \n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frames for train, validation, test\n",
    "# 1) Train\n",
    "train_w2v_data = get_avg_w2v_features(w2v_model, final_train_df, 'train')\n",
    "# 2) Test\n",
    "test_w2v_data = get_avg_w2v_features(w2v_model, final_test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF w2v vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_w2v_features(tf_idf_vect, w2v_model, df, partition_type):\n",
    "    \n",
    "    # set output base directory\n",
    "    out_base_dir = '/home/amd_3/AAIC/ASM_REPO/Processed_data/AMZN_FOOD_REVIW/TFIDF_W2V/'\n",
    "    \n",
    "    # form idf dictionary\n",
    "    idf_dictionary = dict(zip(tf_idf_vect.get_feature_names(), tf_idf_vect.idf_))\n",
    "    \n",
    "    # average Word2Vec\n",
    "    list_of_sent = (df['CleanedText'].apply(lambda x : x.split())).tolist()\n",
    "    \n",
    "    # TF-IDF weighted Word2Vec\n",
    "    tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "    \n",
    "    \n",
    "    tfidf_sent_vectors = list(); # the tfidf-w2v for each sentence/review is stored in this list\n",
    "    \n",
    "    for sent in list_of_sent: # for each review/sentence \n",
    "        \n",
    "        sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "        \n",
    "        weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "        \n",
    "        # process each review\n",
    "        for word in sent: # for each word in a review/sentence\n",
    "            try:\n",
    "                vec = w2v_model.wv[word]\n",
    "                # obtain the tf_idfidf of a word in a sentence/review\n",
    "                tf_idf = sent.count(word) * idf_dictionary[word]\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if weight_sum != 0:\n",
    "            sent_vec /= weight_sum\n",
    "        \n",
    "        # update list\n",
    "        tfidf_sent_vectors.append(sent_vec)\n",
    "        \n",
    "    # create the feature df\n",
    "    col_names = ['dim_' + str(item) for item in range(1, 51)]\n",
    "    feature_df = pd.DataFrame(tfidf_sent_vectors, columns=col_names)\n",
    "    feature_df['Label'] = df['Label'].tolist()\n",
    "    feature_df['Id'] = df['Id']\n",
    "    \n",
    "    # write to disk\n",
    "    feature_df.to_csv(os.path.join(out_base_dir, partition_type + '_tf_w2v.csv'),\n",
    "                      index=False)\n",
    "    # dump the bow model as pickle file\n",
    "    w2v_model_file = open(os.path.join(out_base_dir, 'tfidfw2v_model.pickle'), 'wb')\n",
    "    pickle.dump(w2v_model, w2v_model_file)\n",
    "    w2v_model_file.close()\n",
    "    \n",
    "    return feature_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frames for train, validation, test\n",
    "# 1) Train\n",
    "train_tfw2v_data = get_tfidf_w2v_features(tf_idf_vect, w2v_model, \n",
    "                                          final_train_df, 'train')\n",
    "# 3) Test\n",
    "test_tfw2v_data = get_tfidf_w2v_features(tf_idf_vect, w2v_model,\n",
    "                                         final_test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps Followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%HTML\n",
    "<ul>\n",
    "<li> Basic EDA such as class distribution, common word presence analysis in +ve, -ve reviews\n",
    "<li> Cleaning of raw review text using methods like html tag removal, special charactes removal, stop words\n",
    "     removal, stemming etc.</li>\n",
    "<li> Featurization of cleaned review using four different methods 1) Bag of Words, 2) TFIDF, 3) Avg-W2v\n",
    "     and 4) TFIDF w2v</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
